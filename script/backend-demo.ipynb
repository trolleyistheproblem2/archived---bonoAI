{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import requests\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "import spacy \n",
    "from bertopic import BERTopic\n",
    "import hdbscan\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
    "import vertexai.preview.generative_models as generative_models\n",
    "\n",
    "\n",
    "## all available on conda except google-cloud-aiplatform and google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the API key from the environment variable\n",
    "# openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# # Set the API key\n",
    "# openai.api_key = openai_api_key\n",
    "\n",
    "# #Defining Seed\n",
    "\n",
    "# seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'path_to_your_chat.txt' with the actual path to your exported chat file\n",
    "# If the chat file is in your Google Drive, provide the correct path\n",
    "chat_path = r\"C:\\Users\\Shardul\\OneDrive - London Business School\\coding\\bonoAI\\raw\\demo-chat-2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse each line of the chat file\n",
    "def parse_line(line):\n",
    "    try:\n",
    "        # Split the line into timestamp and message content\n",
    "        timestamp, message = line.strip().split(\" - \", 1)\n",
    "        # Parse the timestamp into a datetime object\n",
    "        timestamp = datetime.strptime(timestamp, \"%m/%d/%y, %I:%M %p\")\n",
    "        # Extract sender and raw text from the message content\n",
    "        sender, raw_text = message.split(\": \", 1)\n",
    "        # Remove '\\n' characters from raw_text\n",
    "        raw_text = raw_text.replace(\"\\\\n\", \"\\n\")\n",
    "        return timestamp, sender, raw_text\n",
    "    except ValueError:\n",
    "        # Skip lines that don't match the expected format\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the chat file line by line and parse each line\n",
    "chat_data = []\n",
    "current_message = None\n",
    "with open(chat_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        timestamp, sender, raw_text = parse_line(line)\n",
    "        if timestamp is not None and sender is not None and raw_text is not None:\n",
    "            if current_message is not None:\n",
    "                chat_data.append(current_message)\n",
    "            current_message = {\"timestamp\": timestamp, \"sender\": sender, \"raw_text\": raw_text}\n",
    "        else:\n",
    "            if current_message is not None:\n",
    "                current_message[\"raw_text\"] += \"\\n\" + line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the parsed chat data\n",
    "df_main = pd.DataFrame(chat_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the \"\\n\" with actual new lines\n",
    "df['raw_text'] = df['raw_text'].replace(r'\\n', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['raw_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional columns based on raw_text column\n",
    "df['link_dummy'] = df['raw_text'].str.contains('http')\n",
    "\n",
    "# Extract HTTP links from raw_text and fill link_value column\n",
    "df['link_value'] = df['raw_text'].str.extract(r'(https?://\\S+)', expand=False)\n",
    "\n",
    "\n",
    "# df['link_label'] = df['link_value'].apply(lambda x: 'Twitter' if x and 'twitter' in x else ('Blog' if x and 'blog' in x else None))\n",
    "# df['link_content'] = df['raw_text']\n",
    "df['plaintext_dummy'] = df['link_dummy'].apply(lambda x: 1 if not x else 0)\n",
    "\n",
    "\n",
    "df['plaintext_content'] = df['raw_text'].where(df['plaintext_dummy'] == 1, None)\n",
    "\n",
    "# Display the DataFrame with additional columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy = df[['sender','link_value', 'plaintext_content', 'timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'link_jina' with appended base URL\n",
    "df_dummy.loc[:,'link_jina'] = df_dummy['link_value'].apply(lambda x: f\"https://r.jina.ai/{x}\" if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_content(link_jina):\n",
    "    if pd.notnull(link_jina):\n",
    "        try:\n",
    "            response = requests.get(link_jina)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummy.loc[:,'link_content'] = df_dummy['link_jina'].apply(get_link_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_dummy.sample(frac=.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is just a dev environment tool, we will remove this in production\n",
    "\n",
    "def remove_nan_rows(df, column_name, percentage=0.99):\n",
    "    null_rows = df[df[column_name].isnull()]\n",
    "    num_rows_to_remove = int(len(null_rows) * percentage)\n",
    "\n",
    "    # Randomly select rows to keep\n",
    "    keep_indices = np.random.choice(null_rows.index, size=len(null_rows) - num_rows_to_remove, replace=False)\n",
    "\n",
    "    # Keep the selected rows and non-null rows\n",
    "    df_filtered = pd.concat([df.loc[keep_indices], df[~df[column_name].isnull()]])\n",
    "\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df_small = remove_nan_rows(df_small.copy(), column_name=\"link_value\", percentage=0.9)\n",
    "print(df_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df_small.iloc[0:5]\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small['link_content'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list_of_dicts(df):\n",
    "  \"\"\"Converts a DataFrame into a list of dictionaries.\n",
    "\n",
    "  Args:\n",
    "      df: A Pandas DataFrame.\n",
    "\n",
    "  Returns:\n",
    "      A list of dictionaries, where each dictionary represents a row in the DataFrame.\n",
    "  \"\"\"\n",
    "def df_to_list_of_dicts(df):\n",
    "    data_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        data_dict = row.to_dict()\n",
    "\n",
    "        # Handle potential None values\n",
    "        if pd.notnull(data_dict.get('plaintext_content')) and pd.notnull(data_dict.get('link_content')):  \n",
    "            data_dict['combined_input'] = data_dict['plaintext_content'] + \" [DELIMITER] \" + data_dict['link_content']\n",
    "\n",
    "        data_list.append(data_dict)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = df_to_list_of_dicts(df_small.copy())  # Use a copy to avoid modifying original\n",
    "print(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [item['link_content'] for item in list_of_dicts if 'link_content' in item]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generation_config = {\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "safety_settings = {\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH ,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH ,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH ,\n",
    "    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH ,\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_test = f\"{prompt_description_instructions}: {content_list}\"  \n",
    "\n",
    "def generate():\n",
    "  vertexai.init(project=\"bonoai-421313\", location=\"us-central1\")\n",
    "  model = GenerativeModel(\"gemini-1.0-pro\")\n",
    "  responses = model.generate_content(\n",
    "      [prompt_test],\n",
    "      generation_config=generation_config,\n",
    "      safety_settings=safety_settings,\n",
    "      stream=True,\n",
    "  )\n",
    "\n",
    "  for response in responses:\n",
    "    print(response.text, end=\"\")\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    \"person\",      # people, including fictional characters\n",
    "    \"fac\",         # buildings, airports, highways, bridges\n",
    "    \"org\",         # organizations, companies, agencies, institutions\n",
    "    \"gpe\",         # geopolitical entities like countries, cities, states\n",
    "    \"loc\",         # non-gpe locations\n",
    "    \"product\",     # vehicles, foods, appareal, appliances, software, toys \n",
    "    \"event\",       # named sports, scientific milestones, historical events\n",
    "    \"work_of_art\", # titles of books, songs, movies\n",
    "    \"law\",         # named laws, acts, or legislations\n",
    "    \"language\",    # any named language\n",
    "    \"date\",        # absolute or relative dates or periods\n",
    "    \"time\",        # time units smaller than a day\n",
    "    \"percent\",     # percentage (e.g., \"twenty percent\", \"18%\")\n",
    "    \"money\",       # monetary values, including unit\n",
    "    \"quantity\",    # measurements, e.g., weight or distance\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=\"bonoai-421313\", location=\"us-central1\")\n",
    "model_name = GenerativeModel(\"gemini-1.0-pro\")\n",
    "max_length_per_chunk = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_metadata_instructions = f\"\"\" \n",
    "\n",
    "Persona: Persona: You are an analytical assistant for a tech-savvy user. Analyze the following content and provide a structured output. Do not include self-referential statements.\n",
    "\n",
    "Instructions: \n",
    "1. Ignore fields such as timestamp and sender\n",
    "2. Return the summary in the form of a raw dictionary in plain English with the following keys. DO NOT USE python or json prefix anywhere \n",
    "* Short Summary: Write a 2 line description of the main content.\n",
    "* Labels: Provide a comma-separated list of relevant tags (e.g., twitter, url, design, website, experimental).\n",
    "* Event: Is this related to an event (Calendar meeting, deadline, etc.)? Answer 'Yes' or 'No'.\n",
    "* Topic:  Identify the primary topic (e.g., Design, Productivity).\n",
    "* Journal: If this content is from a recognized source, provide the name (Newspaper, Media Outlet, Academic Journal).\n",
    "* Action Needed:  Explicitly state any to-dos or actions required from the user.\n",
    "* Author: If possible, identify the author of the content.  \n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_description_instructions = f\"\"\" \n",
    "\n",
    "Persona: Persona: You are an analytical assistant for a tech-savvy user. Analyze the following content and provide a structured summary. \n",
    "\n",
    "Instructions: \n",
    "1. Ignore all meta fields such as timestamp, sender, entities, labels in from the input data\n",
    "2. Do not be self-referential at all, for example adding statements such as 'I hope this summary is helpful. Please let me know if you have any other questions.'\n",
    "3. Return the summary in the form of a raw dictionary in plain English with the key \"Detailed Summary\". \n",
    "4. DO NOT USE 'python' or 'json' prefix anywhere \n",
    "5. Keep all main points, key events and any significant outcomes or conclusions from the article content\n",
    "6. Highlight important data, mention relevant figures involved, and discuss the implications or impact mentioned in the article content\n",
    "\n",
    "\n",
    "\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_row_metadata(data_dict):\n",
    "    \"\"\"Processes a single dictionary, sending content to Gemini and adding the summary.\n",
    "\n",
    "    Args:\n",
    "        data_dict: A dictionary representing a DataFrame row.\n",
    "\n",
    "    Returns:\n",
    "        The modified dictionary with the 'summary' field added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract and Combine Content\n",
    "    if pd.notnull(data_dict.get('plaintext_content')) and pd.notnull(data_dict.get('link_content')):  \n",
    "        combined_input = data_dict['plaintext_content'] + \" [DELIMITER] \" + data_dict['link_content']\n",
    "    elif pd.notnull(data_dict.get('plaintext_content')):\n",
    "        combined_input = data_dict['plaintext_content']\n",
    "    elif pd.notnull(data_dict.get('link_content')):\n",
    "        combined_input = data_dict['link_content'] \n",
    "    else:\n",
    "        combined_input = \"\"  # Or some default handling\n",
    "\n",
    "\n",
    "    # spaCy Named Entity Recognition\n",
    "    doc = nlp(combined_input)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents] \n",
    "    entity_labels = [label for _, label in entities if label in labels]\n",
    "\n",
    "    # Update the dictionary with spaCy results \n",
    "    data_dict[\"entities\"] = entities\n",
    "    data_dict[\"entity_labels\"] = \", \".join(entity_labels)\n",
    "\n",
    "    # Gemini API Call\n",
    "    prompt = f\"{prompt_metadata_instructions}: {combined_input}\"  \n",
    "    responses = model_name.generate_content(\n",
    "        [prompt],\n",
    "        generation_config=generation_config,  # Your generation settings\n",
    "        safety_settings=safety_settings     # Your safety settings\n",
    "    )\n",
    "    summary = responses.text\n",
    "    # summary = response.text\n",
    "\n",
    "    # # MongoDB Structure\n",
    "    # whatsapp_doc = {\n",
    "    #     \"timestamp\": data_dict[\"timestamp\"],  # Assuming you have this field from parsing.\n",
    "    #     \"sender\": data_dict[\"sender\"],\n",
    "    #     \"raw_text\": data_dict[\"plaintext_content\"], \n",
    "    #     \"link_references\": [data_dict[\"link_value\"]]  # If there are multiple links, add them all\n",
    "    # }\n",
    "\n",
    "    # article_doc = {\n",
    "    #     \"url\": data_dict[\"link_value\"],\n",
    "    #     \"content\": data_dict[\"link_content\"], \n",
    "    #     \"summary\": summary \n",
    "    # }\n",
    "\n",
    "\n",
    "\n",
    "    # Add Summary to the Dictionary\n",
    "    data_dict['summary'] = summary\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "# Process Each Dictionary\n",
    "processed_docs = [process_row_metadata(data_dict) for data_dict in list_of_dicts]\n",
    "print(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_into_chunks(text, max_length):\n",
    "    \"\"\"\n",
    "    Divide the text into chunks, each of size less than or equal to max_length.\n",
    "    This function uses nltk to tokenize the text into sentences and then groups\n",
    "    these sentences into chunks.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to divide.\n",
    "        max_length (int): Maximum length of each chunk in characters.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of string chunks, each of length <= max_length.\n",
    "    \"\"\"\n",
    "# Tokenize the text into sentences using nltk\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    chunks = []  # List to hold chunks\n",
    "    current_chunk = \"\"  # String to accumulate sentences into a chunk\n",
    "\n",
    "    # Iterate over each sentence, grouping sentences into chunks\n",
    "    for sentence in sentences:\n",
    "        # Check if adding this sentence would exceed the max_length\n",
    "        if len(current_chunk + (\"\" if current_chunk == \"\" else \" \") + sentence) > max_length:\n",
    "            # If the current chunk + new sentence exceeds max_length, save the current chunk\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence  # Start a new chunk with the current sentence\n",
    "            \n",
    "        else:\n",
    "            # Add the sentence to the current chunk with a space if it's not empty\n",
    "            current_chunk += (\"\" if current_chunk == \"\" else \" \") + sentence\n",
    "    \n",
    "    # Add the last chunk if it contains any text\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk {index + 1}: {chunk}\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(chunk, prompt_description_instructions, model_name, generation_config, safety_settings):\n",
    "    \"\"\"Summarizes a text chunk using Gemini.\n",
    "\n",
    "    Args:\n",
    "        chunk: The text chunk to summarize.\n",
    "        prompt_description_instructions: Instructions to include in the Gemini prompt.\n",
    "        model_name: The Gemini model to use for summarization.\n",
    "        generation_config: Configuration settings for the generation process.\n",
    "        safety_settings: Safety settings for the generation process.\n",
    "\n",
    "    Returns:\n",
    "        The generated summary text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        prompt = f\"{prompt_description_instructions}: {chunk}\"\n",
    "        response = model_name.generate_content(\n",
    "            [prompt],\n",
    "            generation_config=generation_config, \n",
    "            safety_settings=safety_settings \n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        if \"Quota exceeded\" in str(e):\n",
    "            print(\"Quota exceeded, sleeping for 60 seconds...\")\n",
    "            time.sleep(60)  # Sleep for 60 seconds before retrying\n",
    "            return summarize_chunk(chunk, prompt_description_instructions, model_name, generation_config, safety_settings)  # Retry summarization\n",
    "        else:\n",
    "            raise e  # Re-raise the exception if it's not a quota issue \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_document(combined_input, prompt_instructions, model_name, generation_config, safety_settings, max_length_per_chunk):\n",
    "    \"\"\"Summarizes a document in a hierarchical manner using Gemini.\n",
    "\n",
    "    Args:\n",
    "        combined_input: The input text to summarize.\n",
    "        prompt_instructions: Instructions for the Gemini prompts.\n",
    "        model: The Gemini model for summarization.\n",
    "        generation_config: Generation settings for Gemini.\n",
    "        safety_settings: Safety settings for Gemini.\n",
    "        max_length_per_chunk: Maximum length of text chunks.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the final summary ('final_summary') and \n",
    "              the intermediate summaries ('intermediate_summaries'). \n",
    "    \"\"\"\n",
    "\n",
    "    # Divide into chunks\n",
    "    chunks = divide_into_chunks(combined_input, max_length_per_chunk)\n",
    "    \n",
    "    for index, chunk in enumerate(chunks):\n",
    "        print(f\"Chunk in main summary function number {index + 1}: {chunk}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Summarize each chunk\n",
    "    intermediate_summaries = [\n",
    "        summarize_chunk(chunk, prompt_instructions, model_name, generation_config, safety_settings) \n",
    "        for chunk in chunks\n",
    "    ]\n",
    "\n",
    "    print('Intermediate Summary', intermediate_summaries)\n",
    "\n",
    "    \n",
    "\n",
    "    # Concatenate intermediate summaries\n",
    "    concatenated_summary = \" \".join(intermediate_summaries)\n",
    "\n",
    "    print(concatenated_summary)\n",
    "\n",
    "    \n",
    "\n",
    "    # Summarize the concatenated summary\n",
    "    final_summary = summarize_chunk(concatenated_summary, prompt_instructions, model_name, generation_config, safety_settings)\n",
    "\n",
    "    print(final_summary)\n",
    "    \n",
    "\n",
    "    return {\n",
    "        'final_summary': final_summary,\n",
    "        'intermediate_summaries': intermediate_summaries\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Each Dictionary\n",
    "full_description = [summarize_document(content_list, prompt_description_instructions, model_name, generation_config, safety_settings, max_length_per_chunk) for content_list in content_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for description in full_description:\n",
    "    final_summary = description['final_summary']\n",
    "    print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Each Dictionary For Metadata\n",
    "processed_docs = [process_row_metadata(data_dict) for data_dict in list_of_dicts]\n",
    "#processed_list = [process_row(data_dict) for data_dict in list_of_dicts]\n",
    "print(processed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dictionaries\n",
    "merged_docs = [\n",
    "    {**doc, **desc} for doc, desc in zip(processed_docs, full_description)\n",
    "]\n",
    "\n",
    "# Print the merged list to see the output\n",
    "print(merged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_docs\n",
    "\n",
    "\n",
    "# Convert list of dictionaries to a DataFrame\n",
    "df_merged_final = pd.DataFrame(merged_docs)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df_merged_final.to_csv(r'C:\\Users\\Shardul\\OneDrive - London Business School\\coding\\bonoAI\\output\\merged_documents.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Your long document text goes here. It might include several sentences that you need to process in parts.\"\n",
    "max_length = 1000  # maximum length of each chunk\n",
    "chunks_demo = divide_into_chunks(text, max_length)\n",
    "for index, chunk in enumerate(chunks_demo):\n",
    "    print(f\"Chunk {index + 1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the List of Dictionaries to a DataFrame\n",
    "df_processed = pd.DataFrame(processed_docs)\n",
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining entities into a string and then combining with summary\n",
    "df_processed['entities_str'] = df_processed['entities'].apply(lambda x: ', '.join([f\"{ent[0]} ({ent[1]})\" for ent in x]))\n",
    "df_processed['combined_summary_text'] = df_processed['entities_str'] + \" \" + df_processed['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
    "\n",
    "MODEL = \"text-embedding-preview-0409\"\n",
    "TASK = \"QUESTION_ANSWERING\"\n",
    "TITLE = \"Embedding Title\"\n",
    "OUTPUT_DIMENSIONALITY = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_df(df):\n",
    "    BATCH_SIZE = 250  # Adjust based on your model's limits\n",
    "\n",
    "    def process_batch(batch_df):\n",
    "        texts = batch_df['combined_summary_text'].tolist() \n",
    "        embeddings = embed_text(\n",
    "            model_name=MODEL, \n",
    "            task_type=TASK,\n",
    "            texts=texts\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df), BATCH_SIZE):\n",
    "        batch_df = df.iloc[i:i + BATCH_SIZE]\n",
    "        batch_embeddings = process_batch(batch_df)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    df['embedding_vector'] = embeddings\n",
    "    return df\n",
    "\n",
    "\n",
    "## Alternative function \n",
    "# def get_embeddings_df(df):\n",
    "#     # Initialize an empty list to store embeddings\n",
    "#     embeddings = []\n",
    "\n",
    "#     # Iterate over each row in the dataframe using iterrows()\n",
    "#     for index, row in df.iterrows():\n",
    "#         input_text = [row['combined_summary_text']]  # Ensure text is in list format\n",
    "        \n",
    "#         # Embed Text\n",
    "#         embedding = embed_text(\n",
    "#             model_name=MODEL,\n",
    "#             task_type=TASK,\n",
    "#             texts=input_text\n",
    "#         )[0]  # Extract the embedding since there's only one input per call\n",
    "        \n",
    "#         # Append the embedding to the list\n",
    "#         embeddings.append(embedding)\n",
    "\n",
    "#     # Assign the list of embeddings to a new dataframe column\n",
    "#     df['embedding_vector'] = embeddings\n",
    "\n",
    "#     return df\n",
    "\n",
    "# get_embeddings_df(df_processed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embeddings_df(df_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['formatted_embeddings'] = df_processed['embedding_vector'].apply(lambda x: x.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed['formatted_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the column elements are actually lists \n",
    "if isinstance(df_processed[\"formatted_embeddings\"].iloc[0], list):\n",
    "    embeddings_array = np.array(df_processed[\"formatted_embeddings\"].tolist())\n",
    "else:\n",
    "    # Handle cases where the column elements are not lists (e.g., strings)\n",
    "    # ... add logic to convert strings into lists of numbers if needed ...\n",
    "    embeddings_array = np.array(df_processed[\"formatted_embeddings\"].tolist()) # Update this line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Assuming `embeddings_array` should be created or verified\n",
    "# Ensure embeddings_array is properly computed and not empty\n",
    "if embeddings_array.size == 0:\n",
    "    print(\"Error: The embeddings array is empty!\")\n",
    "else:\n",
    "    print(\"Embeddings array is ready for modeling.\")\n",
    "\n",
    "\n",
    "# Check if the data column is empty or has non-string types\n",
    "if df_processed['combined_summary_text'].isnull().any() or not all(isinstance(x, str) for x in df_processed['combined_summary_text']):\n",
    "    print(\"Data error: Check 'combined_summary_text' for nulls or non-string entries.\")\n",
    "else:\n",
    "    print(\"Data is properly formatted.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Below is WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)  # Reduce to 50 dimensions as an example\n",
    "reduced_embeddings = pca.fit_transform(embeddings_array)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=2, min_samples=1, metric='euclidean')\n",
    "labels = clusterer.fit_predict(reduced_embeddings)\n",
    "print(\"Number of clusters found:\", len(np.unique(labels[labels >= 0])))\n",
    "print(\"Labels assigned to data points:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, learning_rate='auto', init='random')\n",
    "reduced_data = tsne.fit_transform(embeddings_array)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=labels, cmap='Spectral', s=50)\n",
    "plt.title('Data Distribution via t-SNE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy data to test UMAP\n",
    "dummy_data = np.random.rand(10, 5)  # 10 samples, 5 features each\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "try:\n",
    "    reduced_embeddings = umap_model.fit_transform(dummy_data)\n",
    "    print(\"UMAP works with dummy data, output:\", reduced_embeddings)\n",
    "except Exception as e:\n",
    "    print(\"UMAP failed on dummy data with error:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'embeddings_array' is your precomputed embeddings and is correctly structured\n",
    "# Ensure n_neighbors is not greater than the number of samples\n",
    "if embeddings_array.shape[0] < umap_model.n_neighbors:\n",
    "    print(f\"Reducing n_neighbors from {umap_model.n_neighbors} to {embeddings_array.shape[0] // 2}\")\n",
    "    umap_model.n_neighbors = max(2, embeddings_array.shape[0] // 2)\n",
    "\n",
    "# Attempt to fit UMAP again\n",
    "try:\n",
    "    reduced_embeddings = umap_model.fit_transform(embeddings_array)\n",
    "    print(\"UMAP successfully reduced dimensionality.\")\n",
    "except Exception as e:\n",
    "    print(\"UMAP failed with adjusted parameters due to:\", str(e))\n",
    "\n",
    "# Use KMeans for clustering\n",
    "kmeans_model = KMeans(n_clusters=10, random_state=42)\n",
    "clusters = kmeans_model.fit_predict(reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "# Adjust UMAP parameters or check input size\n",
    "umap_model = UMAP(n_neighbors=3, n_components=2, min_dist=0.1, metric='euclidean', n_jobs=1)\n",
    "\n",
    "# Try manually fitting UMAP to see if it works outside BERTopic\n",
    "try:\n",
    "    umap_model.fit(embeddings_array)  # Directly fitting to check if UMAP works with given embeddings\n",
    "    print(\"UMAP dimensionality reduction successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"UMAP Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate embeddings array is not empty and contains valid numerical data:\n",
    "if embeddings_array.size == 0:\n",
    "    raise ValueError(\"The embeddings array is empty.\")\n",
    "if np.isnan(embeddings_array).any():\n",
    "    raise ValueError(\"NaN values found in embeddings array.\")\n",
    "if embeddings_array.ndim != 2:\n",
    "    raise ValueError(\"Embeddings array must be two-dimensional.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 10\n",
    "\n",
    "# Load and initialize BERTopic to use KMeans clustering with 8 clusters only.\n",
    "cluster_model = KMeans(n_clusters=n_clusters)\n",
    "topic_model = BERTopic(hdbscan_model=cluster_model)\n",
    "\n",
    "# df is a dataframe. df['title'] is the column of text we're modeling\n",
    "df_processed['topic'], probabilities = topic_model.fit_transform(df_processed['combined_summary_text'], embeddings_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic with embedding_model set to None since we are using precomputed embeddings\n",
    "topic_model = BERTopic(embedding_model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a BERTopic instance\n",
    "topic_model = BERTopic(embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")  # Or your preferred embedding model \n",
    "\n",
    "# Fit the model on your embeddings\n",
    "topics, probabilities = topic_model.fit_transform(df_processed[\"formatted_embeddings\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "topics, probabilities = topic_model.fit_transform(df_processed['formatted_embeddings'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add topic numbers back to the original dataframe\n",
    "df_processed['topic'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Topic Info\n",
    "topic_info = topic_model.get_topic_info()\n",
    "\n",
    "# Print topic information\n",
    "print(topic_info)\n",
    "\n",
    "# To visualize the topics, if feasible depending on your environment (best in Jupyter Notebooks)\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nomic is WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nomic import atlas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project = atlas.map_data(\n",
    "#     embeddings=embeddings_array\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nomic login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomic cred\n",
    "# id: bonodemo\n",
    "# key: nk-LlPaaXJBULXS3hxXWUCkQGt-NsxMpLtXsM737Fd79Wk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #MONGO_URI = \"mongodb+srv://backend-demo-user:mS5qycxBRSPXmVG@bonoaicluster.n8jbhf5.mongodb.net/?retryWrites=true&w=majority&appName=BonoAICluster\"\n",
    "\n",
    "\n",
    "# MONGO_URI = \"mongodb+srv://shardulvaidya95:wMmjqUoEXWSR9J7h@bonoaicluster.n8jbhf5.mongodb.net/?retryWrites=true&w=majority&appName=BonoAICluster\"\n",
    "\n",
    "# # Create a new client and connect to the server\n",
    "# client = MongoClient(MONGO_URI, server_api=ServerApi('1'))\n",
    "# # Send a ping to confirm a successful connection\n",
    "# try:\n",
    "#     client.admin.command('ping')\n",
    "#     print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "\n",
    "\n",
    "# for whatsapp_doc, article_doc in processed_docs:\n",
    "#     whatsapp_result = db.bonoAIWhatsappChats.insert_one(whatsapp_doc)\n",
    "#     print(f\"WhatsApp Chat Inserted with ID: {whatsapp_result.inserted_id}\")\n",
    "\n",
    "#     article_result = db.bonoAIArticles.insert_one(article_doc)\n",
    "#     print(f\"Article Inserted with ID: {article_result.inserted_id}\")\n",
    "\n",
    "\n",
    "# whatsapp_count = db.bonoAIWhatsappChats.count_documents({})  # Pass an empty filter\n",
    "# article_count = db.bonoAIArticles.count_documents({})  # Pass an empty filter\n",
    "\n",
    "# print(f\"WhatsApp Chats Count: {whatsapp_count}\")\n",
    "# print(f\"Articles Count: {article_count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
